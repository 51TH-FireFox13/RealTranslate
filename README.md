# RealTranslate ğŸŒ

**Traduction en temps rÃ©el multilingue** avec dÃ©tection automatique de la voix

Application web de traduction instantanÃ©e multilingue, utilisant la reconnaissance vocale, la traduction automatique et la synthÃ¨se vocale.

## âœ¨ FonctionnalitÃ©s

- ğŸ¤ **DÃ©tection automatique de la voix (VAD)** - Pas de bouton Ã  presser !
- ğŸ”„ **Traduction en temps rÃ©el** - Whisper + GPT-4o-mini / DeepSeek
- ğŸ”Š **SynthÃ¨se vocale automatique** - OpenAI TTS
- ğŸŒ **DÃ©tection gÃ©ographique** - OpenAI (monde) ou DeepSeek (Chine)
- ğŸ“± **Responsive** - Fonctionne sur smartphone, PC, enceintes Bluetooth
- ğŸ”’ **SÃ©curisÃ©** - ClÃ©s API protÃ©gÃ©es cÃ´tÃ© backend

## ğŸ—ï¸ Architecture

```
RealTranslate/
â”œâ”€â”€ frontend/           # Interface utilisateur
â”‚   â”œâ”€â”€ index.html     # Page web principale
â”‚   â””â”€â”€ app.js         # Logique VAD + API calls
â”œâ”€â”€ backend/           # Serveur Node.js
â”‚   â”œâ”€â”€ server.js      # API REST sÃ©curisÃ©e
â”‚   â”œâ”€â”€ package.json   # DÃ©pendances Node
â”‚   â”œâ”€â”€ .env           # ClÃ©s API (Ã  crÃ©er)
â”‚   â””â”€â”€ .env.example   # Template de configuration
â””â”€â”€ README.md          # Ce fichier
```

## ğŸš€ Installation

### PrÃ©requis

- Node.js v18+ installÃ© ([tÃ©lÃ©charger](https://nodejs.org/))
- ClÃ©s API OpenAI et DeepSeek

### 1. Installer les dÃ©pendances

```bash
cd backend
npm install
```

### 2. Configuration des clÃ©s API

CrÃ©ez un fichier `.env` dans le dossier `backend/` :

```bash
cp .env.example .env
```

Ã‰ditez `.env` et ajoutez vos clÃ©s :

```env
OPENAI_API_KEY=sk-votre-cle-openai-ici
DEEPSEEK_API_KEY=sk-votre-cle-deepseek-ici
PORT=3000
```

### 3. DÃ©marrer le serveur

```bash
npm start
```

Le serveur dÃ©marre sur `http://localhost:3000`

## ğŸ“– Utilisation

1. **Ouvrir l'application** : AccÃ©dez Ã  `http://localhost:3000` dans votre navigateur
2. **Autoriser le microphone** : Cliquez sur "Activer le Microphone" et acceptez la permission
3. **Parler** : Parlez simplement en franÃ§ais ou en chinois
4. **Traduction automatique** : L'application dÃ©tecte automatiquement quand vous arrÃªtez de parler, traduit et lit la traduction

## ğŸ¯ Comment Ã§a marche ?

### Flux de traduction

```
1. ğŸ¤ Microphone â†’ DÃ©tection de voix (VAD)
2. ğŸ“ Enregistrement audio â†’ Whisper API (transcription)
3. ğŸŒ Texte â†’ GPT-4o-mini / DeepSeek (traduction)
4. ğŸ”Š Traduction â†’ OpenAI TTS (synthÃ¨se vocale)
5. ğŸ”„ Retour Ã  l'Ã©tape 1
```

### VAD (Voice Activity Detection)

- **Analyse du volume audio** en temps rÃ©el (Web Audio API)
- **DÃ©tection de parole** : volume > seuil pendant > 800ms
- **DÃ©tection de silence** : volume < seuil pendant > 1200ms
- **Auto-arrÃªt** : enregistrement s'arrÃªte automatiquement aprÃ¨s le silence

### SÃ©lection du provider

- **DÃ©tection gÃ©ographique** via headers HTTP (`cf-ipcountry`, etc.)
- **Chine** â†’ DeepSeek API
- **Reste du monde** â†’ OpenAI API

## ğŸ› ï¸ Technologies utilisÃ©es

### Backend
- Node.js + Express
- OpenAI API (Whisper, GPT-4o-mini, TTS)
- DeepSeek API (deepseek-chat)
- Multer (upload audio)

### Frontend
- HTML5 + CSS3 + Vanilla JavaScript
- Web Audio API (VAD)
- MediaRecorder API (enregistrement)
- Fetch API (communication backend)

## ğŸ“± Support des plateformes

| Plateforme | Support | Notes |
|-----------|---------|-------|
| ğŸ–¥ï¸ Desktop (Chrome/Edge) | âœ… Complet | RecommandÃ© |
| ğŸ–¥ï¸ Desktop (Firefox) | âœ… Complet | |
| ğŸ–¥ï¸ Desktop (Safari) | âš ï¸ Partiel | Limitations MediaRecorder |
| ğŸ“± Android (Chrome) | âœ… Complet | |
| ğŸ“± iOS (Safari) | âš ï¸ Partiel | Limitations WebRTC |
| ğŸ”Š Enceintes Bluetooth | âœ… Complet | Via connexion systÃ¨me |

## ğŸ”§ Configuration avancÃ©e

### Ajuster la sensibilitÃ© VAD

Dans `frontend/app.js`, modifiez les paramÃ¨tres :

```javascript
const VAD_CONFIG = {
  VOLUME_THRESHOLD: 0.02,      // Seuil de dÃ©tection (â†‘ = moins sensible)
  SILENCE_DURATION: 1200,      // DurÃ©e silence (ms) avant arrÃªt
  MIN_RECORDING_DURATION: 800  // DurÃ©e minimale d'enregistrement (ms)
};
```

### Changer les voix TTS

Dans `frontend/app.js`, fonction `speakText()` :

```javascript
// Voix disponibles: alloy, echo, fable, onyx, nova, shimmer
const voice = language === 'zh' ? 'nova' : 'onyx';
```

## ğŸ› RÃ©solution de problÃ¨mes

### Le microphone ne fonctionne pas

- VÃ©rifiez les permissions du navigateur (icÃ´ne cadenas dans la barre d'adresse)
- Sur iOS : Safari uniquement, Chrome/Firefox non supportÃ©s
- Essayez HTTPS au lieu de HTTP (requis sur certains navigateurs)

### La traduction est lente

- VÃ©rifiez votre connexion internet
- DeepSeek peut Ãªtre plus lent qu'OpenAI selon votre localisation
- RÃ©duisez `SILENCE_DURATION` pour une rÃ©ponse plus rapide (mais risque de coupure)

### Erreur "API Key invalid"

- VÃ©rifiez que vos clÃ©s sont correctes dans `backend/.env`
- RedÃ©marrez le serveur aprÃ¨s modification du `.env`

### Audio coupÃ© trop tÃ´t

- Augmentez `SILENCE_DURATION` dans `VAD_CONFIG`
- Augmentez `VOLUME_THRESHOLD` si l'environnement est bruyant

## ğŸ” SÃ©curitÃ©

- âœ… ClÃ©s API stockÃ©es cÃ´tÃ© serveur uniquement (jamais exposÃ©es au client)
- âœ… Validation des inputs cÃ´tÃ© backend
- âœ… CORS configurÃ© pour limiter les accÃ¨s
- âœ… Uploads audio limitÃ©s Ã  25MB
- âš ï¸ Pour la production : ajouter HTTPS, rate limiting, authentification

## ğŸ“ License

MIT

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## ğŸ“§ Support

Pour toute question ou problÃ¨me, ouvrez une issue sur GitHub.

---

**DÃ©veloppÃ© avec â¤ï¸ pour faciliter la communication FR â†” ZH**
