// Configuration
const API_BASE_URL = window.location.origin;
const VAD_CONFIG = {
  VOLUME_THRESHOLD: 0.02,      // Seuil de d√©tection de voix
  SILENCE_DURATION: 1200,      // Dur√©e de silence pour arr√™ter (ms)
  MIN_RECORDING_DURATION: 800, // Dur√©e minimale d'enregistrement (ms)
  RECORDING_INTERVAL: 100      // Intervalle d'analyse (ms)
};

// √âtat global
let state = {
  isRecording: false,
  isSpeaking: false,
  mediaRecorder: null,
  audioContext: null,
  analyser: null,
  audioChunks: [],
  recordingStartTime: 0,
  lastSoundTime: 0,
  provider: 'openai',
  audioQueue: [] // Queue pour g√©rer les lectures TTS
};

// √âl√©ments DOM
const elements = {
  permissionModal: document.getElementById('permissionModal'),
  statusBar: document.getElementById('statusBar'),
  statusText: document.getElementById('statusText'),
  frContent: document.getElementById('frContent'),
  zhContent: document.getElementById('zhContent'),
  volumeBar: document.getElementById('volumeBar'),
  providerBadge: document.getElementById('providerBadge'),
  providerName: document.getElementById('providerName')
};

// D√©tection du provider (OpenAI ou DeepSeek)
async function detectProvider() {
  try {
    const response = await fetch(`${API_BASE_URL}/api/detect-region`);
    const data = await response.json();
    state.provider = data.provider;
    elements.providerName.textContent = data.provider.toUpperCase();
    elements.providerBadge.classList.remove('hidden');
    console.log('Provider d√©tect√©:', state.provider);
  } catch (error) {
    console.error('Erreur d√©tection provider:', error);
    state.provider = 'openai'; // Fallback
  }
}

// Mise √† jour du statut visuel
function updateStatus(status, text) {
  elements.statusBar.className = `status-bar ${status}`;
  elements.statusText.textContent = text;
}

// Ajout d'un message dans le panneau
function addMessage(panel, text) {
  const messageDiv = document.createElement('div');
  messageDiv.className = 'message';
  messageDiv.textContent = text;

  const contentElement = panel === 'fr' ? elements.frContent : elements.zhContent;
  contentElement.appendChild(messageDiv);
  contentElement.scrollTop = contentElement.scrollHeight;
}

// Analyse du volume audio (VAD)
function analyzeVolume() {
  if (!state.analyser) return 0;

  const bufferLength = state.analyser.frequencyBinCount;
  const dataArray = new Uint8Array(bufferLength);
  state.analyser.getByteTimeDomainData(dataArray);

  // Calcul du RMS (Root Mean Square) pour le volume
  let sum = 0;
  for (let i = 0; i < bufferLength; i++) {
    const normalized = (dataArray[i] - 128) / 128;
    sum += normalized * normalized;
  }
  const rms = Math.sqrt(sum / bufferLength);

  // Mise √† jour de l'indicateur visuel
  const volumePercent = Math.min(100, rms * 1000);
  elements.volumeBar.style.width = `${volumePercent}%`;

  return rms;
}

// D√©tection automatique de la voix (VAD Loop)
function vadLoop() {
  if (state.isSpeaking) {
    // Ne pas enregistrer pendant la lecture audio
    setTimeout(vadLoop, VAD_CONFIG.RECORDING_INTERVAL);
    return;
  }

  const volume = analyzeVolume();
  const now = Date.now();

  // D√©tection de voix
  if (volume > VAD_CONFIG.VOLUME_THRESHOLD) {
    state.lastSoundTime = now;

    // D√©marrer l'enregistrement si pas d√©j√† en cours
    if (!state.isRecording) {
      startRecording();
    }
  }

  // D√©tection de silence
  if (state.isRecording) {
    const silenceDuration = now - state.lastSoundTime;
    const recordingDuration = now - state.recordingStartTime;

    // Arr√™ter si silence d√©tect√© ET dur√©e minimale atteinte
    if (silenceDuration > VAD_CONFIG.SILENCE_DURATION &&
        recordingDuration > VAD_CONFIG.MIN_RECORDING_DURATION) {
      stopRecording();
    }
  }

  // Continuer la boucle
  setTimeout(vadLoop, VAD_CONFIG.RECORDING_INTERVAL);
}

// D√©marrer l'enregistrement
function startRecording() {
  if (state.isRecording || state.isSpeaking) return;

  console.log('üé§ D√©but enregistrement');
  state.isRecording = true;
  state.audioChunks = [];
  state.recordingStartTime = Date.now();
  state.lastSoundTime = Date.now();

  updateStatus('listening', 'üé§ √âcoute en cours...');

  state.mediaRecorder.start();
}

// Arr√™ter l'enregistrement
function stopRecording() {
  if (!state.isRecording) return;

  console.log('‚è∏Ô∏è Arr√™t enregistrement');
  state.isRecording = false;

  if (state.mediaRecorder.state === 'recording') {
    state.mediaRecorder.stop();
  }
}

// Traitement de l'audio enregistr√©
async function processAudio(audioBlob) {
  // V√©rifier la taille du blob
  if (audioBlob.size < 1000) {
    console.log('‚ö†Ô∏è Audio trop court, ignor√©');
    updateStatus('listening', 'üéß Pr√™t √† √©couter...');
    return;
  }

  updateStatus('translating', 'üîÑ Traduction en cours...');

  try {
    // 1. Transcription avec Whisper
    const transcription = await transcribeAudio(audioBlob);

    if (!transcription || transcription.length < 2) {
      console.log('‚ö†Ô∏è Transcription vide ou trop courte');
      updateStatus('listening', 'üéß Pr√™t √† √©couter...');
      return;
    }

    console.log('üìù Transcription:', transcription);

    // 2. D√©tection de la langue
    const isChinese = /[\u4e00-\u9fff]/.test(transcription);
    const sourceLang = isChinese ? 'zh' : 'fr';
    const targetLang = isChinese ? 'fr' : 'zh';

    // 3. Traduction
    const translation = await translateText(transcription, targetLang);
    console.log('üåê Traduction:', translation);

    // 4. Affichage
    if (sourceLang === 'fr') {
      addMessage('fr', transcription);
      addMessage('zh', translation);
    } else {
      addMessage('zh', transcription);
      addMessage('fr', translation);
    }

    // 5. Text-to-Speech de la traduction
    updateStatus('speaking', 'üîä Lecture audio...');
    await speakText(translation, targetLang);

  } catch (error) {
    console.error('‚ùå Erreur traitement:', error);
    updateStatus('idle', '‚ö†Ô∏è Erreur de traitement');
    setTimeout(() => {
      updateStatus('listening', 'üéß Pr√™t √† √©couter...');
    }, 2000);
  }
}

// Transcription audio avec Whisper
async function transcribeAudio(audioBlob) {
  const formData = new FormData();
  formData.append('audio', audioBlob, 'audio.webm');

  const response = await fetch(`${API_BASE_URL}/api/transcribe`, {
    method: 'POST',
    body: formData
  });

  if (!response.ok) {
    throw new Error(`Erreur transcription: ${response.statusText}`);
  }

  const data = await response.json();
  return data.text?.trim();
}

// Traduction du texte
async function translateText(text, targetLanguage) {
  const response = await fetch(`${API_BASE_URL}/api/translate`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json'
    },
    body: JSON.stringify({
      text,
      targetLanguage,
      provider: state.provider
    })
  });

  if (!response.ok) {
    throw new Error(`Erreur traduction: ${response.statusText}`);
  }

  const data = await response.json();
  return data.translatedText;
}

// Text-to-Speech
async function speakText(text, language) {
  state.isSpeaking = true;

  // Choisir la voix selon la langue
  const voice = language === 'zh' ? 'nova' : 'onyx';

  try {
    const response = await fetch(`${API_BASE_URL}/api/speak`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ text, voice })
    });

    if (!response.ok) {
      throw new Error(`Erreur TTS: ${response.statusText}`);
    }

    const audioBlob = await response.blob();
    const audioUrl = URL.createObjectURL(audioBlob);
    const audio = new Audio(audioUrl);

    return new Promise((resolve, reject) => {
      audio.onended = () => {
        state.isSpeaking = false;
        updateStatus('listening', 'üéß Pr√™t √† √©couter...');
        URL.revokeObjectURL(audioUrl);
        resolve();
      };

      audio.onerror = (error) => {
        state.isSpeaking = false;
        updateStatus('listening', 'üéß Pr√™t √† √©couter...');
        reject(error);
      };

      audio.play();
    });

  } catch (error) {
    state.isSpeaking = false;
    updateStatus('listening', 'üéß Pr√™t √† √©couter...');
    throw error;
  }
}

// Initialisation du microphone et du syst√®me audio
async function initializeAudio() {
  try {
    // Obtenir l'acc√®s au microphone
    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        sampleRate: 16000
      }
    });

    // Configuration de l'analyseur audio (VAD)
    state.audioContext = new (window.AudioContext || window.webkitAudioContext)();
    state.analyser = state.audioContext.createAnalyser();
    state.analyser.fftSize = 2048;
    state.analyser.smoothingTimeConstant = 0.8;

    const source = state.audioContext.createMediaStreamSource(stream);
    source.connect(state.analyser);

    // Configuration du MediaRecorder
    state.mediaRecorder = new MediaRecorder(stream, {
      mimeType: 'audio/webm;codecs=opus'
    });

    state.mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        state.audioChunks.push(event.data);
      }
    };

    state.mediaRecorder.onstop = async () => {
      const audioBlob = new Blob(state.audioChunks, { type: 'audio/webm' });
      state.audioChunks = [];
      await processAudio(audioBlob);
    };

    // Tout est pr√™t
    elements.permissionModal.classList.add('hidden');
    updateStatus('listening', 'üéß Pr√™t √† √©couter...');

    // D√©marrer la boucle VAD
    vadLoop();

    console.log('‚úÖ Syst√®me audio initialis√©');

  } catch (error) {
    console.error('‚ùå Erreur initialisation audio:', error);
    updateStatus('idle', '‚ö†Ô∏è Erreur microphone');
  }
}

// Demande de permission microphone
async function requestMicrophonePermission() {
  await detectProvider();
  await initializeAudio();
}

// Initialisation au chargement
window.addEventListener('load', () => {
  console.log('üöÄ RealTranslate charg√©');
  updateStatus('idle', 'Cliquez pour activer le microphone');
});

// Gestion du r√©veil de l'application (mobile)
document.addEventListener('visibilitychange', () => {
  if (document.visibilityState === 'visible' && state.audioContext) {
    state.audioContext.resume();
  }
});
